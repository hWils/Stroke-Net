# -*- coding: utf-8 -*-
"""Raw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1geuua09zIwyj-GWlTclGq9kLHoi3EAWa
"""

# -*- coding: utf-8 -*-
"""
This takes the data for one ppt in one session and organises it into
two arrays, one for left and one for right motor imagery data. 
All data not related to the MI trials is disregarded.
"""
import numpy as np
import glob
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.activations import sigmoid, relu
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

import scipy.io as spio
import matplotlib.pyplot as plt

def figure(History, legend):
  ####################### PLOT TRAINING VS VALIDATION ######################
  ########## Accuracy ###########
  acc = History.history['acc']
  val_acc = History.history['val_acc']
  loss = History.history['loss']
  val_loss = History.history['val_loss']

  plt.plot(acc)
  plt.plot(val_acc)
  plt.title('model accuracy')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(legend, loc='upper left')
  plt.grid()
  plt.show()

  ########## Loss ###########
  plt.plot(loss)
  plt.plot(val_loss)
  plt.title('model loss')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  try:
    loss_no_reg = History.history['categorical_crossentropy']
    val_loss_no_reg = History.history['val_categorical_crossentropy']
    plt.plot(loss_no_reg)
    plt.plot(val_loss_no_reg)
    plt.legend(legend + [legend[0]+' sin reg', legend[1]+' sin reg'], loc='upper left')
  except:
    plt.legend(legend, loc='upper left')

  plt.grid()
  plt.show()

from google.colab import files
uploaded = files.upload()

# #change path
path_TRAIN = r'/content/P3_post_training.mat' # use your path
path_test = r'/content/P3_post_test.mat' # use your path
path = r'/content'
files_TRAIN = glob.glob(path + "/*training.mat")
files_test = glob.glob(path + "/*test.mat")

mat_TRAIN = spio.loadmat(path_TRAIN, squeeze_me=True)
mat_test = spio.loadmat(path_test, squeeze_me=True)

def extract(mat):
  # path = 'C:\Users\Rosana\Documents\GTEC\stroke\P1_pre_training.mat'
  # mat = spio.loadmat(path, squeeze_me=True)

  print("The keys of this data dictionary for this ppt are :", mat.keys())
  triggers = mat['trig']
  fs = mat['fs']
  eeg_data = mat['y']
  print("There are ", len(triggers), " triggers")
  print(max(triggers)) 
  print("sampling frequency is ", mat['fs'])
  print("Length of data set is ", len(eeg_data))




  # get indices that indicate change in blocks, such as 000s then 1111s in order to split these
  v=np.asarray(triggers)
  indices = np.where(np.diff(v,prepend=np.nan))[0]
  #print(indices)

  listvalues = []
  listclasses = []
  ind = 0
  for ind in range(0, len(indices)-1):
      listclasses.append(triggers[indices[ind]]) # means we know which class each block relates to
      listvalues.append(eeg_data[indices[ind]:indices[ind+1]]) # gets the block as a subset


  # splits the data into the two separate conditions left and right, removes 0 triggers
  # this also means each trial should now be equal length so can be put into an array
  left_class = []
  left_value = []
  right_class = []
  right_value = []

  for i,x in enumerate(listclasses):
      if x ==1:
          left_class.append(listclasses[i])
          left_value.append(listvalues[i])
      elif x ==-1:
          right_class.append(listclasses[i])
          right_value.append(listvalues[i])
          
  # only keep data with relevant time points, 2 till 8 seconds
  start_point = 2*fs
  end_point = 8*fs
  left_value_trial = []
  right_value_trial = []
  for trial in left_value:
      left_value_trial.append(trial[start_point:end_point])
  for trial in right_value:
      right_value_trial.append(trial[start_point:end_point])
          

  left_data = np.asarray(left_value_trial)
  right_data = np.asarray(right_value_trial)
  print(left_data.shape, right_data.shape)
  left_labels = np.ones(len(left_data))
  right_labels = np.zeros(len(right_data))
  print(right_labels)

  return left_data, left_labels, right_data, right_labels

left_data, left_labels, right_data, right_labels = extract(mat_TRAIN)

print('X right: (samples, time, features) =', right_data.shape)
print('Y right: (sample labels) =', right_labels.shape)
print('X left: (samples, time, features) =', left_data.shape)
print('Y left: (sample labels) =', left_labels.shape)

x_TRAIN_orig = np.concatenate((right_data, left_data), axis = 0)
y_TRAIN_orig = np.concatenate((right_labels, left_labels), axis = 0)

left_data, left_labels, right_data, right_labels = extract(mat_test)

x_test_orig = np.concatenate((right_data, left_data), axis = 0)
y_test_orig = np.concatenate((right_labels, left_labels), axis = 0)

def windowed(x,windows_size, overlap_factor = 0):
  fs = 256
  pieces = len(x[0])//windows_size
  temp1 = []
  temp2 = []
  overlap = int(windows_size*overlap_factor)

  for j in range(len(x)):
    temp1 = []
    i = 0
    init_index = 0 
    last_index = windows_size
    while len(x[j][init_index:last_index]) == windows_size:
      temp_data = x[j][init_index:last_index]#[0:-1:10,:]
      temp1.append(np.concatenate((np.mean(temp_data, axis = 0), np.std(temp_data, axis = 0)), axis = 0) ) #, np.var(temp_data, axis = 0) ])
      init_index = init_index + windows_size - overlap
      last_index = last_index + windows_size - overlap
      
    #temp2.append(np.asarray(temp1).reshape([-1,32]))
    temp2.append(np.asarray(temp1))
    
  return np.asarray(temp2)

x_TRAIN_orig2 = windowed(x_TRAIN_orig, 500)
x_test_orig2 = windowed(x_test_orig, 500)

scaler = MinMaxScaler()
max_value = np.max(x_TRAIN_orig2, axis=(0,1))
min_value = np.min(x_TRAIN_orig2, axis=(0,1))

x_TRAIN = (x_TRAIN_orig2-min_value)/(max_value-min_value)
x_test = (x_test_orig2-min_value)/(max_value-min_value)

x_TRAIN_orig.shape

x_TRAIN_orig2.shape

plt.plot(y_TRAIN_orig)

np.max(x_test)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(x_TRAIN.shape[1],x_TRAIN.shape[2]), dropout=0),
  tf.keras.layers.LSTM(32, dropout=0),
  tf.keras.layers.Dense(1, activation = sigmoid),
])

lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-6 * 10**(epoch / 20))
optimizer = tf.keras.optimizers.Adam(lr=1e-6)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=["acc"])
history = model.fit(x_TRAIN, y_TRAIN_orig, epochs=100, callbacks=[lr_schedule])

plt.semilogx(history.history["lr"], history.history["loss"])
#plt.axis([1e-8, 1e-4, 0, 30])

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(32, return_sequences=True, input_shape=(x_TRAIN.shape[1],x_TRAIN.shape[2]) , dropout=0),
  tf.keras.layers.LSTM(32, dropout=0),
  tf.keras.layers.Dense(1, activation = sigmoid),
])

model.summary()

optimizer = tf.keras.optimizers.Adam(lr=2e-3)
model.compile(loss='binary_crossentropy',
              optimizer=optimizer,
              metrics=["acc"])
# Early stopping
stopping = EarlyStopping(monitor='val_loss',min_delta=0,mode='auto',patience=50, restore_best_weights=True)

history = model.fit(x_TRAIN, y_TRAIN_orig, callbacks=[stopping] , validation_data = (x_test, y_test_orig), epochs=2000)

figure(history, ['train', 'test'])

plt.plot(model.predict_classes(x_test))
plt.plot(y_test_orig)
model.evaluate(x_test, y_test_orig)

